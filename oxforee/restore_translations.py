import json
import urllib.request
import urllib.parse
import time
import sys
import re

# Set encoding for Windows console
if sys.stdout.encoding.lower() != 'utf-8':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except AttributeError:
        pass

# Configuration
INPUT_FILE = 'oxford-3000-filled.json'
OUTPUT_FILE = 'oxford-3000-filled.json' # Overwrite with cleaned data
JS_OUTPUT_FILE = 'data.js'

# High-Quality Manual Map for Common/Problematic Words
MANUAL_MAP = {
    "a": "‡∏´‡∏ô‡∏∂‡πà‡∏á / ‡∏Å",
    "an": "‡∏´‡∏ô‡∏∂‡πà‡∏á / ‡∏Å",
    "the": "‡∏ô‡∏µ‡πà / ‡∏ô‡∏±‡πâ‡∏ô",
    "about": "‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö / ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì",
    "across": "‡∏Ç‡πâ‡∏≤‡∏° / ‡∏ï‡∏•‡∏≠‡∏î",
    "against": "‡∏ï‡πà‡∏≠‡∏ï‡πâ‡∏≤‡∏ô / ‡∏û‡∏¥‡∏á / ‡∏Å‡∏±‡∏ö",
    "all": "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î / ‡∏ó‡∏∏‡∏Å‡∏™‡∏¥‡πà‡∏á",
    "and": "‡πÅ‡∏•‡∏∞",
    "as": "‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô / ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏∞",
    "at": "‡∏ó‡∏µ‡πà",
    "aim": "‡∏à‡∏∏‡∏î‡∏°‡∏∏‡πà‡∏á‡∏´‡∏°‡∏≤‡∏¢ / ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢",
    "addition": "‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏° / ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°",
    "alcohol": "‡πÅ‡∏≠‡∏•‡∏Å‡∏≠‡∏Æ‡∏≠‡∏•‡πå",
    "ankle": "‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πâ‡∏≤",
    "assess": "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô",
    "atmosphere": "‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏Å‡∏≤‡∏®",
    "awful": "‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å / ‡∏ô‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ß",
    "benefit": "‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå / ‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå",
    "blame": "‡∏ï‡∏≥‡∏´‡∏ô‡∏¥ / ‡∏Å‡∏•‡πà‡∏≤‡∏ß‡πÇ‡∏ó‡∏©",
    "blank": "‡∏ß‡πà‡∏≤‡∏á / ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á",
    "block": "‡∏Å‡∏±‡πâ‡∏ô / ‡∏ö‡∏•‡πá‡∏≠‡∏Å / ‡∏Å‡πâ‡∏≠‡∏ô",
    "bond": "‡∏Ç‡πâ‡∏≠‡∏ú‡∏π‡∏Å‡∏°‡∏±‡∏î / ‡∏û‡∏±‡∏ô‡∏ò‡∏∞",
    "boyfriend": "‡πÅ‡∏ü‡∏ô‡∏´‡∏ô‡∏∏‡πà‡∏°",
    "breast": "‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏Å / ‡πÄ‡∏ï‡πâ‡∏≤‡∏ô‡∏°",
    "call": "‡πÄ‡∏£‡∏µ‡∏¢‡∏Å / ‡πÇ‡∏ó‡∏£",
    "capital": "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á / ‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏∏‡∏ô",
    "coast": "‡∏ä‡∏≤‡∏¢‡∏ù‡∏±‡πà‡∏á",
    "combine": "‡∏ú‡∏™‡∏° / ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô",
    "convince": "‡πÇ‡∏ô‡πâ‡∏°‡∏ô‡πâ‡∏≤‡∏ß / ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏ä‡∏∑‡πà‡∏≠",
    "actually": "‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÅ‡∏•‡πâ‡∏ß",
    "according to": "‡∏ï‡∏≤‡∏° / ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà",
    "active": "‡∏Ñ‡∏•‡πà‡∏≠‡∏á‡πÅ‡∏Ñ‡∏•‡πà‡∏ß / ‡∏Å‡∏£‡∏∞‡∏ï‡∏∑‡∏≠‡∏£‡∏∑‡∏≠‡∏£‡πâ‡∏ô",
    "advance": "‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤ / ‡∏û‡∏±‡∏í‡∏ô‡∏≤",
    "acceptable": "‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ",
    "access": "‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á / ‡∏ó‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏≤",
    "accident": "‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏",
    "accommodation": "‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏Å",
    "accompany": "‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô / ‡∏°‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö",
    "account": "‡∏ö‡∏±‡∏ç‡∏ä‡∏µ / ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô",
    "accurate": "‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ / ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á",
    "accuse": "‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏´‡∏≤ / ‡∏Å‡∏•‡πà‡∏≤‡∏ß‡πÇ‡∏ó‡∏©",
    "achieve": "‡∏ö‡∏£‡∏£‡∏•‡∏∏ / ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à",
    "achievement": "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à",
    "acknowledge": "‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö / ‡∏£‡∏±‡∏ö‡∏ó‡∏£‡∏≤‡∏ö",
    "acquire": "‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö / ‡πÑ‡∏î‡πâ‡∏°‡∏≤",
    "act": "‡πÅ‡∏™‡∏î‡∏á / ‡∏Å‡∏£‡∏∞‡∏ó‡∏≥ / ‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢",
    "action": "‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥",
    "after": "‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å",
    "already": "‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß",
    "apple": "‡πÅ‡∏≠‡∏õ‡πÄ‡∏õ‡∏¥‡πâ‡∏•",
    "association": "‡∏™‡∏°‡∏≤‡∏Ñ‡∏°",
    "balance": "‡∏™‡∏°‡∏î‡∏∏‡∏• / ‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠",
    "ban": "‡∏´‡πâ‡∏≤‡∏° / ‡∏™‡∏±‡πà‡∏á‡∏´‡πâ‡∏≤‡∏°",
    "base": "‡∏ê‡∏≤‡∏ô / ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô",
    "based": "‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏° / ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö",
    "beer": "‡πÄ‡∏ö‡∏µ‡∏¢‡∏£‡πå",
}

def is_thai(text):
    """Check if text contains primarily Thai characters"""
    if not text: return False
    # Thai Unicode range: \u0e00-\u0e7f
    thai_chars = re.findall(r'[\u0e00-\u0e7f]', text)
    return len(thai_chars) > (len(text) * 0.5) # At least 50% Thai

def clean_translation(text):
    """Clean up translation text"""
    if not text: return ""
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    # Remove excessive whitespace
    text = " ".join(text.split())
    # Remove suspicious suffixes or prefixes often found in bad scrapes
    text = re.sub(r'Name=.*Comment', '', text)
    text = re.sub(r'usa\. kgm', '', text)
    text = re.sub(r'kumbinsihin', '', text) # Tagalog
    # Remove romanized Thai if it follows Thai text
    # e.g. "‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πâ‡∏≤ kÃÑƒ•x th√™ƒÅ" -> "‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πâ‡∏≤"
    # This is tricky, but we can try to find the start of latin chars after Thai
    match = re.search(r'([\u0e00-\u0e7f]+)\s+[a-zÃÑ]', text)
    if match:
        text = match.group(1).strip()
    
    # If it's too long, it's probably a definition instead of a translation
    if len(text) > 80:
        # Try to take the first sentence or part before / or ,
        parts = re.split(r'[/,;.]', text)
        if parts:
            text = parts[0].strip()
            
    return text.strip()

def fetch_translation(text):
    """Fetch Thai translation for a given word using Google Translate"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=en&tl=th&dt=t&q={urllib.parse.quote(text)}"
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req) as response:
            data = json.loads(response.read().decode('utf-8'))
            if data and data[0]:
                translated = "".join([part[0] for part in data[0] if part[0]])
                return clean_translation(translated)
    except Exception as e:
        pass
    return None

def main():
    print(f"üöÄ Starting Translation Restoration...")
    
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    updated_count = 0
    cleaned_count = 0
    
    for i, item in enumerate(data):
        word = item['word'].strip()
        current_trans = item.get('translate', '').strip()
        
        needs_update = False
        new_trans = None
        
        # 1. Check for manual override
        if word.lower() in MANUAL_MAP:
            new_trans = MANUAL_MAP[word.lower()]
            if new_trans != current_trans:
                needs_update = True
        
        # 2. Check for empty or corrupted current translation
        elif not current_trans or current_trans == "-" or not is_thai(current_trans) or len(current_trans) > 100:
            needs_update = True
            print(f"[{i+1}/3000] Word '{word}' needs repair. Current: '{current_trans}'")
            new_trans = fetch_translation(word)
            if not new_trans:
                print(f"   ‚ö†Ô∏è Failed to fetch translation for '{word}'")
            else:
                print(f"   ‚úÖ New: '{new_trans}'")
                time.sleep(0.5) # Rate limiting
        
        # 3. Basic cleaning for existing ones
        else:
            cleaned = clean_translation(current_trans)
            if cleaned != current_trans:
                item['translate'] = cleaned
                cleaned_count += 1
        
        if needs_update and new_trans:
            item['translate'] = new_trans
            updated_count += 1
            
        # Periodic save
        if (i + 1) % 100 == 0:
            print(f"üìä Progress: {i+1}/3000 words processed...")
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

    # Final save
    print(f"\n‚úÖ Restoration Complete!")
    print(f"   - Words updated: {updated_count}")
    print(f"   - Words cleaned: {cleaned_count}")
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    # Sync with data.js
    js_content = f"const OXFORD_DATA = {json.dumps(data, indent=2, ensure_ascii=False)};"
    with open(JS_OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(js_content)
    print(f"üíæ Synced with {JS_OUTPUT_FILE}")

if __name__ == "__main__":
    main()
